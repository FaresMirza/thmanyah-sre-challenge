apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: prometheus-ns
data:
  alerts.yml: |
    groups:
      # ===== Service Availability Alerts =====
      - name: service_availability
        interval: 30s
        rules:
          - alert: ServiceDown
            expr: up{job=~"api-service|auth-service|image-service"} == 0
            for: 2m
            labels:
              severity: critical
              component: "{{ $labels.job }}"
            annotations:
              summary: "Service {{ $labels.job }} is down"
              description: "{{ $labels.job }} has been down for more than 2 minutes."

          - alert: HighPodRestartRate
            expr: rate(kube_pod_container_status_restarts_total{namespace=~"api-ns|auth-ns|image-ns"}[15m]) > 0.1
            for: 5m
            labels:
              severity: warning
              component: kubernetes
            annotations:
              summary: "High pod restart rate in {{ $labels.namespace }}"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently."

      # ===== API Service Alerts =====
      - name: api_service
        interval: 30s
        rules:
          - alert: APIHighErrorRate
            expr: |
              (sum(rate(http_requests_total{job="api-service",status_code=~"5.."}[5m])) by (instance) 
              / sum(rate(http_requests_total{job="api-service"}[5m])) by (instance)) > 0.05
            for: 5m
            labels:
              severity: critical
              component: api-service
            annotations:
              summary: "API service has high error rate"
              description: "API service error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

          - alert: APIHighLatency
            expr: |
              histogram_quantile(0.95, 
                sum(rate(http_request_duration_seconds_bucket{job="api-service"}[5m])) by (le)
              ) > 1
            for: 5m
            labels:
              severity: warning
              component: api-service
            annotations:
              summary: "API service has high latency"
              description: "API service p95 latency is {{ $value }}s (threshold: 1s)"

          - alert: APILowThroughput
            expr: sum(rate(http_requests_total{job="api-service"}[5m])) < 0.1
            for: 10m
            labels:
              severity: info
              component: api-service
            annotations:
              summary: "API service has low throughput"
              description: "API service is receiving {{ $value }} requests/second"

      # ===== Auth Service Alerts =====
      - name: auth_service
        interval: 30s
        rules:
          - alert: AuthServiceHighErrorRate
            expr: |
              (sum(rate(http_requests_total{job="auth-service",status=~"Internal Server Error|Bad Gateway|Service Unavailable|Gateway Timeout"}[5m])) by (instance)
              / sum(rate(http_requests_total{job="auth-service"}[5m])) by (instance)) > 0.05
            for: 5m
            labels:
              severity: critical
              component: auth-service
            annotations:
              summary: "Auth service has high error rate"
              description: "Auth service error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

          - alert: AuthServiceHighLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket{job="auth-service"}[5m])) by (le)
              ) > 0.5
            for: 5m
            labels:
              severity: warning
              component: auth-service
            annotations:
              summary: "Auth service has high latency"
              description: "Auth service p95 latency is {{ $value }}s (threshold: 0.5s)"

          - alert: HighFailedLoginAttempts
            expr: |
              sum(rate(http_requests_total{job="auth-service",endpoint="/login",status="Unauthorized"}[5m])) by (instance) > 5
            for: 5m
            labels:
              severity: warning
              component: auth-service
            annotations:
              summary: "High failed login attempts detected"
              description: "Auth service is experiencing {{ $value }} failed logins/second on {{ $labels.instance }}"

      # ===== Image Service Alerts =====
      - name: image_service
        interval: 30s
        rules:
          - alert: ImageServiceHighErrorRate
            expr: |
              (sum(rate(http_requests_total{job="image-service",status=~"5.."}[5m])) by (instance)
              / sum(rate(http_requests_total{job="image-service"}[5m])) by (instance)) > 0.05
            for: 5m
            labels:
              severity: critical
              component: image-service
            annotations:
              summary: "Image service has high error rate"
              description: "Image service error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

          - alert: ImageProcessingHighLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket{job="image-service"}[5m])) by (le)
              ) > 3
            for: 5m
            labels:
              severity: warning
              component: image-service
            annotations:
              summary: "Image service has high processing latency"
              description: "Image service p95 latency is {{ $value }}s (threshold: 3s)"

      # ===== Database Alerts =====
      - name: database
        interval: 30s
        rules:
          - alert: PostgreSQLDown
            expr: up{job="postgres"} == 0
            for: 2m
            labels:
              severity: critical
              component: database
            annotations:
              summary: "PostgreSQL database is down"
              description: "PostgreSQL has been unreachable for more than 2 minutes."

          - alert: HighDatabaseConnections
            expr: |
              pg_stat_database_numbackends{datname!~"template.*|postgres"} 
              / pg_settings_max_connections > 0.8
            for: 5m
            labels:
              severity: warning
              component: database
            annotations:
              summary: "High database connection usage"
              description: "Database connection usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      # ===== Resource Utilization Alerts =====
      - name: resource_utilization
        interval: 30s
        rules:
          - alert: HighCPUUsage
            expr: |
              sum(rate(container_cpu_usage_seconds_total{namespace=~"api-ns|auth-ns|image-ns|data-ns"}[5m])) by (namespace, pod)
              / sum(container_spec_cpu_quota{namespace=~"api-ns|auth-ns|image-ns|data-ns"} 
              / container_spec_cpu_period{namespace=~"api-ns|auth-ns|image-ns|data-ns"}) by (namespace, pod) > 0.8
            for: 5m
            labels:
              severity: warning
              component: kubernetes
            annotations:
              summary: "High CPU usage in {{ $labels.namespace }}"
              description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }} (threshold: 80%)"

          - alert: HighMemoryUsage
            expr: |
              sum(container_memory_working_set_bytes{namespace=~"api-ns|auth-ns|image-ns|data-ns"}) by (namespace, pod)
              / sum(container_spec_memory_limit_bytes{namespace=~"api-ns|auth-ns|image-ns|data-ns"}) by (namespace, pod) > 0.85
            for: 5m
            labels:
              severity: warning
              component: kubernetes
            annotations:
              summary: "High memory usage in {{ $labels.namespace }}"
              description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }} (threshold: 85%)"

          - alert: PodCrashLooping
            expr: |
              rate(kube_pod_container_status_restarts_total{namespace=~"api-ns|auth-ns|image-ns|data-ns"}[15m]) > 0
            for: 5m
            labels:
              severity: critical
              component: kubernetes
            annotations:
              summary: "Pod is crash looping"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping"

      # ===== Storage Alerts =====
      - name: storage
        interval: 30s
        rules:
          - alert: PersistentVolumeHighUsage
            expr: |
              (kubelet_volume_stats_used_bytes{namespace=~"data-ns|prometheus-ns"}
              / kubelet_volume_stats_capacity_bytes{namespace=~"data-ns|prometheus-ns"}) > 0.8
            for: 5m
            labels:
              severity: warning
              component: storage
            annotations:
              summary: "PVC {{ $labels.persistentvolumeclaim }} usage is high"
              description: "PVC usage in {{ $labels.namespace }} is {{ $value | humanizePercentage }} (threshold: 80%)"

      # ===== Network Alerts =====
      - name: networking
        interval: 30s
        rules:
          - alert: IngressHighErrorRate
            expr: |
              sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m])) by (ingress)
              / sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) > 0.05
            for: 5m
            labels:
              severity: warning
              component: ingress
            annotations:
              summary: "Ingress {{ $labels.ingress }} has high error rate"
              description: "Ingress error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

          - alert: HighNetworkErrors
            expr: |
              rate(container_network_transmit_errors_total{namespace=~"api-ns|auth-ns|image-ns"}[5m]) > 10
            for: 5m
            labels:
              severity: warning
              component: networking
            annotations:
              summary: "High network errors in {{ $labels.namespace }}"
              description: "Pod {{ $labels.pod }} is experiencing {{ $value }} network errors/second"

      # ===== HPA Alerts =====
      - name: autoscaling
        interval: 30s
        rules:
          - alert: HPAMaxedOut
            expr: |
              kube_horizontalpodautoscaler_status_current_replicas{namespace=~"api-ns|auth-ns|image-ns"}
              / kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"api-ns|auth-ns|image-ns"} >= 1
            for: 15m
            labels:
              severity: warning
              component: autoscaling
            annotations:
              summary: "HPA {{ $labels.horizontalpodautoscaler }} has reached max replicas"
              description: "HPA in {{ $labels.namespace }} has been at maximum capacity for 15 minutes"

          - alert: HPAScalingDisabled
            expr: |
              kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false"} == 1
            for: 5m
            labels:
              severity: warning
              component: autoscaling
            annotations:
              summary: "HPA {{ $labels.horizontalpodautoscaler }} scaling is disabled"
              description: "HPA scaling is disabled in {{ $labels.namespace }}"

      # ===== Prometheus Self-Monitoring =====
      - name: prometheus_self
        interval: 30s
        rules:
          - alert: PrometheusConfigReloadFailed
            expr: prometheus_config_last_reload_successful == 0
            for: 5m
            labels:
              severity: critical
              component: prometheus
            annotations:
              summary: "Prometheus configuration reload failed"
              description: "Prometheus configuration reload has failed"

          - alert: PrometheusTargetDown
            expr: up == 0
            for: 5m
            labels:
              severity: warning
              component: prometheus
            annotations:
              summary: "Prometheus target {{ $labels.job }} is down"
              description: "Prometheus cannot scrape {{ $labels.job }} on {{ $labels.instance }}"
